#!/usr/bin/env python3
"""æµ‹è¯•ä»»åŠ¡ç”Ÿæˆå™¨åŠŸèƒ½

æµ‹è¯•Kafkaä»»åŠ¡ç”Ÿæˆå™¨çš„å„ç§åŠŸèƒ½ï¼š
1. å•ä¸ªä»»åŠ¡ç”Ÿæˆ
2. æ‰¹é‡ä»»åŠ¡ç”Ÿæˆ
3. Kafkaæ¶ˆæ¯å‘é€
4. OBSæ–‡ä»¶æ‰«æ
"""

import json
import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch
from typing import List, Dict, Any

from chroma_ollama.logging import get_logger
from chroma_ollama.config import config

logger = get_logger(__name__)


def test_task_generation():
    """æµ‹è¯•ä»»åŠ¡ç”ŸæˆåŠŸèƒ½"""
    print("\nğŸ“‹ æµ‹è¯•ä»»åŠ¡ç”ŸæˆåŠŸèƒ½...")
    
    try:
        from chroma_ollama.task_generator import TaskGenerator
        
        # åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
        with patch('chroma_ollama.task_generator.KafkaProducerHelper') as mock_producer, \
             patch('chroma_ollama.task_generator.ObsHelper') as mock_obs, \
             patch('chroma_ollama.task_generator.config') as mock_config:
            
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•é…ç½®
            from chroma_ollama.config import Config
            test_config = Config.get_default_config()
            mock_config.kafka = test_config.kafka
            mock_config.obs = test_config.obs
            
            # æ¨¡æ‹ŸKafkaç”Ÿäº§è€…
            mock_producer_instance = MagicMock()
            mock_producer.return_value = mock_producer_instance
            
            # æ¨¡æ‹ŸOBSåŠ©æ‰‹
            mock_obs_instance = MagicMock()
            mock_obs.return_value = mock_obs_instance
            
            # æ¨¡æ‹ŸOBSæ–‡ä»¶åˆ—è¡¨
            mock_objects = [
                {"key": "data/file1.txt", "size": 1024, "lastModified": "2025-08-25T12:00:00Z"},
                {"key": "data/file2.csv", "size": 2048, "lastModified": "2025-08-25T12:01:00Z"},
                {"key": "logs/app.log", "size": 4096, "lastModified": "2025-08-25T12:02:00Z"}
            ]
            mock_obs_instance.list_objects.return_value = mock_objects
            
            # åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨
            generator = TaskGenerator()
            
            print("âœ… ä»»åŠ¡ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•å•ä¸ªä»»åŠ¡ç”Ÿæˆ
            single_task = generator.generate_single_task(
                file_key="test/single.txt",
                bucket="test-bucket",
                dest_bucket="output-bucket"
            )
            
            assert single_task["key"] == "test/single.txt"
            assert single_task["bucket"] == "test-bucket"
            assert single_task["dest_bucket"] == "output-bucket"
            assert "generated_at" in single_task
            print("âœ… å•ä¸ªä»»åŠ¡ç”Ÿæˆæµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•ä»æ–‡ä»¶åˆ—è¡¨ç”Ÿæˆä»»åŠ¡
            file_list = ["data/file1.txt", "data/file2.csv"]
            tasks_from_list = generator.generate_task_from_file_list(
                file_list=file_list,
                bucket="source-bucket"
            )
            
            assert len(tasks_from_list) == 2
            assert tasks_from_list[0]["key"] == "data/file1.txt"
            assert tasks_from_list[0]["bucket"] == "source-bucket"
            print("âœ… æ–‡ä»¶åˆ—è¡¨ä»»åŠ¡ç”Ÿæˆæµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•ä»OBSåˆ—è¡¨ç”Ÿæˆä»»åŠ¡
            tasks_from_obs = generator.generate_task_from_obs_list(
                bucket="test-bucket",
                prefix="data/",
                max_files=10
            )
            
            assert len(tasks_from_obs) == 3  # æ¨¡æ‹Ÿè¿”å›3ä¸ªæ–‡ä»¶
            assert all("generated_at" in task for task in tasks_from_obs)
            print("âœ… OBSæ‰«æä»»åŠ¡ç”Ÿæˆæµ‹è¯•é€šè¿‡")
            
    except ImportError as e:
        print(f"âš ï¸ ä»»åŠ¡ç”Ÿæˆå™¨æµ‹è¯•è·³è¿‡ï¼ˆæ¨¡å—æœªæ‰¾åˆ°ï¼‰: {e}")
    except Exception as e:
        print(f"âŒ ä»»åŠ¡ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True


def test_kafka_producer():
    """æµ‹è¯•Kafkaç”Ÿäº§è€…åŠŸèƒ½"""
    print("\nğŸ“¤ æµ‹è¯•Kafkaç”Ÿäº§è€…åŠŸèƒ½...")
    
    try:
        from chroma_ollama.utils import KafkaProducerHelper
        
        # æ¨¡æ‹ŸKafkaç”Ÿäº§è€…
        with patch('chroma_ollama.utils.Producer') as mock_producer_class, \
             patch('chroma_ollama.utils.config') as mock_config:
            
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•é…ç½®
            from chroma_ollama.config import Config
            test_config = Config.get_default_config()
            mock_config.kafka = test_config.kafka
            
            mock_producer_instance = MagicMock()
            mock_producer_class.return_value = mock_producer_instance
            
            # åˆ›å»ºç”Ÿäº§è€…åŠ©æ‰‹
            producer = KafkaProducerHelper()
            
            if producer.producer:
                print("âœ… Kafkaç”Ÿäº§è€…åˆå§‹åŒ–æˆåŠŸ")
                
                # æµ‹è¯•å‘é€æ¶ˆæ¯
                test_message = {
                    "key": "test/file.txt",
                    "bucket": "test-bucket",
                    "generated_at": "2025-08-25T12:00:00Z"
                }
                
                producer.send_notification(
                    topic="test-topic",
                    message=test_message,
                    key="test-file"
                )
                
                # éªŒè¯produceæ–¹æ³•è¢«è°ƒç”¨
                mock_producer_instance.produce.assert_called_once()
                mock_producer_instance.flush.assert_called_once()
                
                print("âœ… Kafkaæ¶ˆæ¯å‘é€æµ‹è¯•é€šè¿‡")
            else:
                print("â„¹ï¸ Kafkaç”Ÿäº§è€…æœªå¯ç”¨ï¼ˆé…ç½®æœªæä¾›ï¼‰")
                
    except ImportError as e:
        print(f"âš ï¸ Kafkaç”Ÿäº§è€…æµ‹è¯•è·³è¿‡ï¼ˆæ¨¡å—æœªæ‰¾åˆ°ï¼‰: {e}")
    except Exception as e:
        print(f"âŒ Kafkaç”Ÿäº§è€…æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True


def test_message_formats():
    """æµ‹è¯•æ¶ˆæ¯æ ¼å¼"""
    print("\nğŸ“‹ æµ‹è¯•æ¶ˆæ¯æ ¼å¼...")
    
    # æµ‹è¯•ä»»åŠ¡æ¶ˆæ¯æ ¼å¼
    task_message = {
        "key": "data/input.csv",
        "bucket": "source-bucket", 
        "dest_bucket": "output-bucket",
        "dest_key": "processed/input.csv.json",
        "size": 1024,
        "last_modified": "2025-08-25T12:00:00Z",
        "generated_at": "2025-08-25T12:05:00Z"
    }
    
    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ["key", "generated_at"]
    for field in required_fields:
        assert field in task_message, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
    
    print("âœ… ä»»åŠ¡æ¶ˆæ¯æ ¼å¼éªŒè¯é€šè¿‡")
    
    # æµ‹è¯•ç»“æœé€šçŸ¥æ ¼å¼
    result_message = {
        "timestamp": "2025-08-25T12:30:00Z",
        "worker_type": "single_process",
        "status": "success",
        "source": {
            "bucket": "source-bucket",
            "key": "data/input.csv"
        },
        "destination": {
            "bucket": "output-bucket",
            "key": "processed/input.csv.json"
        },
        "file_info": {
            "input_size": 1024,
            "output_size": 2048,
            "input_size_formatted": "1.0 KB",
            "output_size_formatted": "2.0 KB"
        }
    }
    
    # éªŒè¯ç»“æœæ¶ˆæ¯ç»“æ„
    assert result_message["status"] in ["success", "failed"]
    assert "source" in result_message
    assert "destination" in result_message
    assert "file_info" in result_message
    
    print("âœ… ç»“æœæ¶ˆæ¯æ ¼å¼éªŒè¯é€šè¿‡")
    
    return True


def test_workflow_integration():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹é›†æˆ"""
    print("\nğŸ”„ æµ‹è¯•å·¥ä½œæµç¨‹é›†æˆ...")
    
    # æ¨¡æ‹Ÿå®Œæ•´å·¥ä½œæµç¨‹
    workflow_steps = [
        "1. æ‰«æOBSæ¡¶è·å–æ–‡ä»¶åˆ—è¡¨",
        "2. ç”Ÿæˆå¤„ç†ä»»åŠ¡æ¶ˆæ¯", 
        "3. å‘é€ä»»åŠ¡åˆ°Kafka Topic",
        "4. Workeræ¶ˆè´¹ä»»åŠ¡æ¶ˆæ¯",
        "5. ä¸‹è½½æ–‡ä»¶å¹¶å¤„ç†",
        "6. ä¸Šä¼ ç»“æœåˆ°ç›®æ ‡æ¡¶",
        "7. å‘é€å¤„ç†ç»“æœé€šçŸ¥",
        "8. æ¸…ç†æœ¬åœ°ä¸´æ—¶æ–‡ä»¶"
    ]
    
    print("ğŸ“‹ å®Œæ•´å·¥ä½œæµç¨‹æ­¥éª¤:")
    for step in workflow_steps:
        print(f"  {step}")
    
    # æ¨¡æ‹Ÿå·¥ä½œæµç¨‹æ•°æ®æµ
    workflow_data = {
        "obs_scan": {
            "bucket": "input-files",
            "prefix": "data/2024/",
            "files_found": 150,
            "total_size": "2.5 GB"
        },
        "task_generation": {
            "tasks_created": 150,
            "batch_size": 50,
            "time_taken": "2.3s"
        },
        "kafka_messages": {
            "topic": "file-processing-tasks",
            "messages_sent": 150,
            "success_rate": "100%"
        },
        "worker_processing": {
            "workers": 4,
            "avg_processing_time": "45s",
            "success_rate": "98%"
        },
        "result_notifications": {
            "topic": "file-processing-results", 
            "notifications_sent": 150,
            "success_count": 147,
            "failed_count": 3
        }
    }
    
    print("\nğŸ“Š å·¥ä½œæµç¨‹ç»Ÿè®¡:")
    for stage, stats in workflow_data.items():
        print(f"  {stage.replace('_', ' ').title()}:")
        for key, value in stats.items():
            print(f"    {key.replace('_', ' ').title()}: {value}")
    
    print("âœ… å·¥ä½œæµç¨‹é›†æˆæµ‹è¯•å®Œæˆ")
    
    return True


def generate_sample_configurations():
    """ç”Ÿæˆç¤ºä¾‹é…ç½®"""
    print("\nâš™ï¸ ç”Ÿæˆç¤ºä¾‹é…ç½®...")
    
    # ä»»åŠ¡ç”Ÿæˆå™¨é…ç½®ç¤ºä¾‹
    task_gen_config = {
        "description": "ä»»åŠ¡ç”Ÿæˆå™¨é…ç½®ç¤ºä¾‹",
        "environment_variables": {
            "SOURCE_BUCKET": "input-data-bucket",
            "FILE_PREFIX": "data/2024/08/",
            "DEST_BUCKET": "processed-data-bucket", 
            "MAX_FILES": "1000",
            "KAFKA_TOPIC_TASKS": "file-processing-tasks",
            "KAFKA_ENABLE_PRODUCER": "true"
        },
        "usage_examples": [
            "# æ‰¹é‡ç”Ÿæˆä»»åŠ¡",
            "SOURCE_BUCKET=input-data FILE_PREFIX=logs/ pixi run generate-tasks",
            "",
            "# å•ä¸ªæ–‡ä»¶ä»»åŠ¡",
            "SINGLE_FILE=important.csv SOURCE_BUCKET=urgent-data pixi run generate-single-task"
        ]
    }
    
    # Workeré…ç½®ç¤ºä¾‹
    worker_config = {
        "description": "Workeré…ç½®ç¤ºä¾‹",
        "environment_variables": {
            "KAFKA_TOPIC_TASKS": "file-processing-tasks",
            "KAFKA_RESULT_TOPIC": "file-processing-results",
            "KAFKA_GROUP_ID": "obs-workers",
            "POOL_SIZE": "4",
            "CLEANUP_FILES": "true",
            "CLEANUP_ON_ERROR": "false"
        },
        "usage_examples": [
            "# å¯åŠ¨å•è¿›ç¨‹Worker",
            "pixi run worker",
            "",
            "# å¯åŠ¨å¤šè¿›ç¨‹Worker",
            "POOL_SIZE=8 pixi run pool-worker"
        ]
    }
    
    print("ğŸ“‹ ä»»åŠ¡ç”Ÿæˆå™¨é…ç½®:")
    print(json.dumps(task_gen_config, indent=2, ensure_ascii=False))
    
    print("\nğŸ“‹ Workeré…ç½®:")
    print(json.dumps(worker_config, indent=2, ensure_ascii=False))
    
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•Kafkaä»»åŠ¡ç”Ÿæˆå™¨...")
    print("=" * 60)
    
    tests = [
        ("ä»»åŠ¡ç”ŸæˆåŠŸèƒ½", test_task_generation),
        ("Kafkaç”Ÿäº§è€…", test_kafka_producer),
        ("æ¶ˆæ¯æ ¼å¼", test_message_formats),
        ("å·¥ä½œæµç¨‹é›†æˆ", test_workflow_integration),
        ("ç¤ºä¾‹é…ç½®", generate_sample_configurations)
    ]
    
    passed_tests = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed_tests += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ“‹ åŠŸèƒ½æ€»ç»“:")
        print("  âœ… å•ä¸ªå’Œæ‰¹é‡ä»»åŠ¡ç”Ÿæˆ")
        print("  âœ… Kafkaæ¶ˆæ¯ç”Ÿäº§è€…") 
        print("  âœ… OBSæ–‡ä»¶æ‰«æé›†æˆ")
        print("  âœ… å®Œæ•´å·¥ä½œæµç¨‹æ”¯æŒ")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–")
        return 1


if __name__ == "__main__":
    exit(main())
